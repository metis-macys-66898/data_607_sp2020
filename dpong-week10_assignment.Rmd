---
title: "Week 10 Assignment (D607)"
author: "Dennis Pong"
date: "4/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install.packages("gutenbergr")

library(tidyverse)
library(jsonlite)
library(knitr)
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(gutenbergr) 
```

Start by getting the primary example code from Chapter 2 working, will then extend it with a different corpus, and incorporate at least one additional sentiment lexicon  


### 2.1 The sentiments dataset
```{r}
# need to install package textdata in order to run get_sentiments on afinn
# install.packages("textdata")
get_sentiments("afinn")


```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

### 2.2 Sentiment analysis with inner join


```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```



```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

jane_austen_sentiment
```


```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

### 2.3 Comparing the three sentiment dictionaries

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice
```

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(pride_prejudice %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          pride_prejudice %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```


```{r}
# sentiment of the lexicon nrc
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

# sentiment of the lexicon bing
get_sentiments("bing") %>% 
  count(sentiment)
```

### 2.4 Most common positive and negative words

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r implement-custom-stop-words-list}
custom_stop_words <- bind_rows(tibble(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

### 2.5 Wordclouds

```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```


### 2.6 Looking at units beyond just words

```{r}
PandP_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

PandP_sentences$sentence[2]

```

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  top_n(1) %>%
  ungroup()
```

#### These are the chapters with the most sad words in each book, normalized for number of words in the chapter. What is happening in these chapters? In Chapter 43 of Sense and Sensibility Marianne is seriously ill, near death, and in Chapter 34 of Pride and Prejudice Mr. Darcy proposes for the first time (so badly!). Chapter 46 of Mansfield Park is almost the end, when everyone learns of Henry’s scandalous adultery, Chapter 15 of Emma is when horrifying Mr. Elton proposes, and in Chapter 21 of Northanger Abbey Catherine is deep in her Gothic faux fantasy of murder, etc. Chapter 4 of Persuasion is when the reader gets the full flashback of Anne refusing Captain Wentworth and how sad she was and what a terrible mistake she realized it to be.



#### Now it's my turn to use a different corpus and incorporate an additional set of sentiment lexicon - loughran.
```{r browse-the-gutenberg_works}
gutenberg_works()
```

#### decided to use the corpus from the title "A Sentimental Journey Through France and Italy" by Sterne, Laurence in the bookshelf of Harvard Classics/Best Books Ever Listings/Banned Books from Anne Haight's list.

```{r}
A_Sentimental_Journey_Thru_Fr_and_It <- gutenberg_download(804)
```

```{r}
A_Sentimental_Journey_Thru_Fr_and_It
# found that the book has a repeated work CALAIS. 

tidy_book <- A_Sentimental_Journey_Thru_Fr_and_It %>%
              mutate(linenumber = row_number(),
              chapter = cumsum(str_detect(text, regex("^CALAIS", ignore_case = TRUE)))) %>%
              ungroup() %>%
              unnest_tokens(word, text)
```



```{r check-how-many-chapters-are-there}
unique(tidy_book$chapter)
# summary stats of linenumber which is the row_number()
summary(tidy_book$linenumber)
```

```{r}

tidy_sentiment <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(gutenberg_id, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

tidy_sentiment
```

```{r}
# plots of sentiment per gutenberg_id
ggplot(tidy_sentiment, aes(index, sentiment, fill = gutenberg_id)) +
  geom_col(show.legend = F) +
  facet_wrap(~gutenberg_id, ncol = 2, scales = "free_x")

```

```{r}
# Comparing the four sentiment dictionaries
afinn <- tidy_book %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

```

```{r}
bing_and_nrc_and_lough <- bind_rows(tidy_book %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                             tidy_book %>% 
                                inner_join(get_sentiments("nrc") %>% 
                                             filter(sentiment %in% c("positive", 
                                                                     "negative")))
                                                                %>%
                                                                mutate(method = "NRC"),
                            tidy_book %>%
                            inner_join(get_sentiments("loughran")) %>%
                            mutate(method = "Loughran")
                            ) %>%                            
                          count(method, index = linenumber %/% 80, sentiment) %>%
                          spread(sentiment, n, fill = 0) %>%
                          mutate(sentiment = positive - negative)



```

```{r}
bind_rows(afinn, 
          bing_and_nrc_and_lough) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

#### What stood out to be is that AFINN and Bing et al. matched in terms of patterns. NRC is a little bit different. Loughran is totally negative, which is opposite from the other three.


```{r}
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)
```



```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```

### Most common positive and negative words
```{r}
bing_word_counts <- tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

#### NO NEED to add custom stop words; just use stop_words 
#### Wordclouds
```{r}
tidy_book %>%
 anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

```{r}
tidy_book %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_book %>%
  group_by(gutenberg_id, chapter) %>%
  summarize(words = n())

# instead of removing chapter 0, I included it 
# I sorted the following results by ratio of highest negative words in the chapter
negative_ratios <- tidy_book %>%
  semi_join(bingnegative) %>%
  group_by(gutenberg_id, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("gutenberg_id", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  arrange(desc(ratio)) %>%
  top_n(n()) %>%
  ungroup()

negative_ratios
```
#### looking at top 8 out of 16 (15+1) most negative
```{r}

 ggplot(head(negative_ratios, 8), aes(x = reorder(chapter, - ratio), y = ratio, fill = ratio)) +
  geom_col(show.legend = TRUE) +
  labs(x = "Chapter", y = "Negative Ratios", fill = "Negative Ratios") +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(round(100 * ratio,2),"%")), position=position_dodge(width=0.9), vjust=-0.25)
```

#### looking at top 8 most positive

```{r}
positive_ratios <- tidy_book %>%
  semi_join(bingnegative) %>%
  group_by(gutenberg_id, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("gutenberg_id", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  arrange(ratio) %>%
  top_n(n()) %>%
  ungroup()

positive_ratios
```

```{r}

 ggplot(head(positive_ratios, 8), aes(x = reorder(chapter, ratio), y = ratio, fill = ratio)) +
  geom_col(show.legend = TRUE) +
  labs(x = "Chapter", y = "Positive Ratios", fill = "Positive Ratios") +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(round(100 * ratio,2),"%")), position=position_dodge(width=0.9), vjust=-0.25)
```

#### Chapter 4 turns out to be the most negative in terms of sentiment, at 5.11% while Chapter 12 turns out to be the most positive. I intentionally picked this book called A Sentimental Journey Through France and Italy. I looked forward to seeing more negative terms, which is shown in the Loughran lexicon. It turned out I picked the right lexicon to explore. It probably has a wider (lengthier) list of negative words to match with.  
  
  
  
  
Citation: “Silge, Julia, and David Robinson. Text mining with R: A tidy approach.” O’Reilly Media, Inc.“, 2017.”